# Einf√ºhrung

## Willkommen zum ü§ó Kurs!

<Youtube id="00GKzGyWFEs" />

In diesem Kurs lernst du verschiedene Teilbereiche der maschinellen Verarbeitung nat√ºrlicher Sprache (engl. Natural Language Processing, NLP) - im Deutschen auch als Maschinelle Sprachverarbeitung oder Computerlinguistik (CL) bezeichnet - unter Verwendung der Bibliotheken des √ñkosystems von [Hugging Face](https://huggingface.co/) kennen: die [ü§ó Transformers-](https://github.com/huggingface/transformers), die [ü§ó Datasets-](https://github.com/huggingface/datasets), die [ü§ó Tokenizers-](https://github.com/huggingface/tokenizers) sowie die [ü§ó Accelerate-Bibliotheken](https://github.com/huggingface/accelerate) als auch der [Hugging Face Hub](https://huggingface.co/models). Der Kurs ist komplett kostenlos und frei von Werbung.


## Was erwartet dich?

Hier ein kurzer √úberblick √ºber den Kurs:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- Die Kapitel 1 bis 4 geben eine Einf√ºhrung in die wichtigsten Konzepte der ü§ó Transformers-Bibliothek. Am Ende dieses Teils des Kurses wirst du mit der Funktionsweise von Transformer-Modellen vertraut sein und wissen, wie du ein Modell aus dem [Hugging Face Hub](https://huggingface.co/models) verwendest, es auf einem Datensatz feintunst und deine Ergebnisse mit anderen auf dem Hub teilst!
- In den Kapiteln 5 bis 8 lernst du die Grundlagen der ü§ó Datasets- und ü§ó Tokenizers-Bibliotheken kennen, bevor du in die typischen Problemstellungen des NLP eintauchst. Am Ende dieses Teils wirst du in der Lage sein, die g√§ngisten Problemstellungen im NLP selbstst√§ndig zu l√∂sen.
- Die Kapitel 9 bis 12 gehen √ºber den Bereich des NLP hinaus und zeigen, wie Transformer-Modelle f√ºr Aufgaben bei der Verarbeitung gesprochener Sprache (engl. Speech Processing) und im Bereich Computer Vision (im Deutschen ungef√§hr mit computerbasiertem Sehen zu √ºbersetzen) eingesetzt werden k√∂nnen. Nebenbei lernst du, wie du eigene Versionen deiner Modelle zu Demonstrationszwecken erstellen und sie mit anderen teilen kannst, und wie du sie f√ºr Produktionsumgebungen optimierst. Am Ende dieses Teils wirst du in der Lage sein, die ü§ó Transformers-Bibliothek auf (fast) jede Problemstellung, die dir im Bereich des Maschinellen Lernens begegnen, anzuwenden!

Dieser Kurs:

* Erfordert gute Kenntnisse in Python
* Sollte am besten nach einem Einf√ºhrungskurs in Deep Learning gemacht werden, wie z. B. [fast.ai's Kurs](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) oder eines der von [DeepLearning.AI](https://www.deeplearning.ai/) entwickelten Kursprogramme
* Setzt keine Vorkenntnisse in [PyTorch](https://pytorch.org/) oder [TensorFlow](https://www.tensorflow.org/) voraus, obwohl es hilfreich ist, wenn du bereits mit ihnen vertraut sein solltest.

Nachdem du diesen Kurs abgeschlossen hast, empfehlen wir dir den [Spezialisierungskurs Natural Language Processing von DeepLearning.AI](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), der eine breite Palette traditioneller NLP-Modelle wie Naive Bayes und LSTMs abdeckt, bei denen es sich lohnt, sich mit ihnen vertraut zu machen!

## Wer sind wir?

√úber die Autorinnen und Autoren:

**Matthew Carrigan** ist Machine Learning Engineer bei Hugging Face. Er lebt in der irischen Hauptstadt Dublin und hat zuvor als Machine Learning Engineer bei Parse.ly und als Post-Doktorand am Trinity College Dublin gearbeitet. Er glaubt nicht, dass wir eine k√ºnstliche allgemeine Intelligenz (engl. Artificial General Intelligence, AGI) durch eine zunehmende Skalierung bestehender Architekturen erreichen werden, hat aber dennoch die Hoffnung, dass Roboter auf dem Weg zur Unsterblichkeit sind.

**Lysandre Debut** ist Machine Learning Engineer bei Hugging Face und arbeitet bereits seit Entstehung an der ü§ó Transformers-Bibliothek mit. Sein Ziel ist es, NLP f√ºr alle zug√§nglich zu machen, indem er Tools entwickelt, die eine sehr einfache API bieten.

**Sylvain Gugger** ist Research Engineer bei Hugging Face und einer der Hauptverantwortlichen f√ºr die Pflege der ü§ó Transformers-Bibliothek. Zuvor war er Research Scientist bei fast.ai und hat zusammen mit Jeremy Howard das Buch _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ verfasst. Seine Forschung ist darauf ausgerichtet, Deep Learning zug√§nglicher zu machen. Hierf√ºr entwickelt und verbessert er Techniken, mit denen Modelle auch bei begrenzter Ressourcenausstattung auf schnelle Weise trainiert werden k√∂nnen.

**Merve Noyan** ist Developer Advocate bei Hugging Face und arbeitet daran, Tools zu entwickeln und Inhalte zu erstellen, die Maschinelles Lernen f√ºr jeden zug√§nglich machen.

**Lucile Saulnier** ist Machine Learning Engineer bei Hugging Face und entwickelt und unterst√ºtzt die Nutzung von Open-Source-Tools. Au√üerdem ist sie aktiv an vielen Forschungsprojekten im Bereich des NLP beteiligt, z. B. an kollaborativem Training und BigScience.

**Lewis Tunstall** ist Machine Learning Engineer bei Hugging Face, und konzentriert sich darauf, Open-Source-Tools zu entwickeln und sie der breiten Community zug√§nglich zu machen. Zudem ist er Mitverfasser des O'Reilly-Buches [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/).

**Leandro von Werra** ist Machine Learning Engineer im Open-Source-Team von Hugging Face und ebenfalls einer der Autoren des O'Reilly-Buches [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). Er hat mehrere Jahre praktische Erfahrung darin gesammelt, NLP-Projekte in die Produktion zu bringen, und dabei den gesamten ML-Stack beackert.

Bist du bereit, loszulegen? In diesem Kapitel lernst du
* wie man die Funktion `pipeline()` benutzt, um NLP-Aufgaben wie Textgenerierung und Klassifizierung zu l√∂sen,
* mehr √ºber die Transformer-Architektur und
* wie zwischen Encoder-, Decoder- und Encoder-Decoder-basierten Architekturen und -Anwendungsf√§llen unterschieden werden kann.
