# Einf√ºhrung

## Willkommen zum ü§ó Kurs!

<Youtube id="00GKzGyWFEs" />

In diesem Kurs lernst du verschiedene Teilbereiche der maschinellen Verarbeitung nat√ºrlicher Sprache (engl. Natural Language Processing, NLP), im Deutschen auch als Computerlinguistik (CI) bezeichnet, unter Verwendung der Bibliotheken des [Hugging Face](https://huggingface.co/)-√ñkosystems kennen: [ü§ó Transformers](https://github.com/huggingface/transformers), [ü§ó Datasets](https://github.com/huggingface/datasets), [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) und [ü§ó Accelerate](https://github.com/huggingface/accelerate) als auch der [Hugging Face Hub](https://huggingface.co/models). Der Kurs ist vollkommen kostenlos und frei von Werbung.


## Was erwartet dich?

Hier ein kurzer √úberblick √ºber den Kurs:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Brief overview of the chapters of the course.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Brief overview of the chapters of the course.">
</div>

- Die Kapitel 1 bis 4 geben eine Einf√ºhrung in die wichtigsten Konzepte der ü§ó Transformers-Bibliothek. Am Ende dieses Teils des Kurses wirst du mit der Funktionsweise von Transformer-Modellen vertraut sein und wissen, wie du ein Modell aus dem [Hugging Face Hub](https://huggingface.co/models) verwendest, es auf einem Datensatz feintunst(#M#) und deine Ergebnisse mit anderen auf dem Hub teilst!
- In den Kapiteln 5 bis 8 lernst du die Grundlagen der ü§ó Datasets- und ü§ó Tokenizers-Bibliotheken kennen, bevor du in die typischen Problemstellungen des NLP eintauchst. Am Ende dieses Teils wirst du in der Lage sein, die g√§ngisten Problemstellungen im NLP selbstst√§ndig zu l√∂sen.
- Die Kapitel 9 bis 12 gehen √ºber den Bereich des NLP hinaus und zeigen, wie Transformer-Modelle f√ºr Aufgaben in der Verarbeitung von Sprache (engl. Speech Processing) und im Bereich Computer Vision eingesetzt werden k√∂nnen. Nebenbei lernst du, wie du eigene Versionen(#M#) deiner Modelle zu Demonstrationszwecken erstellst und mit anderen teilst und wie du sie f√ºr Produktionsumgebungen optimierst. Am Ende dieses Teils wirst du in der Lage sein, die ü§ó Transformers-Bibliothek auf (fast) jede Problemstellung im maschinellen Lernen anzuwenden!

Dieser Kurs:

* Erfordert gute Kenntnisse in Python
* Sollte am besten nach einem Einf√ºhrungskurs in Deep Learning gemacht werden, wie z. B. [fast.ai's Kurs](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) oder eines der von [DeepLearning.AI](https://www.deeplearning.ai/) entwickelten Kursprogramme
* Setzt keine Vorkenntnisse in [PyTorch](https://pytorch.org/) oder [TensorFlow](https://www.tensorflow.org/) voraus, obwohl es hilfreich ist, wenn du bereits mit ihnen vertraut sein solltest.

Nachdem du diesen Kurs abgeschlossen hast, empfehlen wir dir den [Spezialisierungskurs Natural Language Processing von DeepLearning.AI](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), der eine breite Palette traditioneller NLP-Modelle wie Naive Bayes und LSTMs abdeckt, die es wert sind, sich mit ihnen vertraut zu machen!

## Wer sind wir?

√úber die Autor/innen:

**Matthew Carrigan** ist Machine Learning Engineer bei Hugging Face. Er lebt in der irischen Hauptstadt Dublin und hat zuvor als Machine Learning Engineer bei Parse.ly und als Post-Doktorand am Trinity College Dublin gearbeitet. Er glaubt nicht, dass wir eine k√ºnstliche allgemeine Intelligenz (engl. Artificial General Intelligence, AGI) durch eine zunehmende Skalierung bestehender Architekturen erreichen werden, hat aber trotzdem die Hoffnung, dass sich Roboter unsterblich machen(#M unsterblich werden; but has high hopes for robot immortality regardless#).

**Lysandre Debut** ist Machine Learning Engineer bei Hugging Face und arbeitet bereits seit Entstehung an der ü§ó Transformers-Bibliothek mit. Sein Ziel ist es, NLP f√ºr alle zug√§nglich zu machen, indem er Tools entwickelt, die eine sehr einfache API bieten.

**Sylvain Gugger** ist Research Engineer bei Hugging Face und einer der Hauptverantwortlichen f√ºr die Pflege der ü§ó Transformers-Bibliothek. Zuvor war er Research Scientist bei fast.ai und hat zusammen mit Jeremy Howard _[Deep Learning for Coders with fastai and PyTorch](https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/)_ verfasst. Der Schwerpunkt seiner Forschung ist es, Deep Learning zug√§nglicher zu machen. Hierf√ºr entwickelt und verbessert er Techniken, mit denen Modelle auch bei begrenzter Ressourcenausstattung schnell trainiert werden k√∂nnen.

**Merve Noyan** ist Developer Advocate bei Hugging Face und arbeitet daran, Tools zu entwickeln und Inhalte zu erstellen, die maschinelles Lernen f√ºr jeden zug√§nglich machen.

**Lucile Saulnier** ist Machine Learning Engineer bei Hugging Face und entwickelt und unterst√ºtzt die Nutzung von Open-Source-Tools. Au√üerdem ist sie aktiv an vielen Forschungsprojekten im Bereich des NLP beteiligt, z. B. an kollaborativem Training und BigScience.

**Lewis Tunstall** ist ein Machine Learning Engineer bei Hugging Face, der sich darauf konzentriert, Open-Source-Tools zu entwickeln und sie der breiten Community zug√§nglich zu machen. Zudem ist er Mitverfasser des O'Reilly-Buches [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/).

**Leandro von Werra** ist Machine Learning Engineer im Open-Source-Team von Hugging Face und ebenfalls einer der Autoren des O'Reilly-Buches [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098103231/). Er hat mehrere Jahre praktische Erfahrung darin gesammelt, NLP-Projekte in die Produktion zu bringen, und dabei den gesamten Stack des maschinellen Lernens beackert(#M He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack#).

Bist du bereit, loszulegen? In diesem Kapitel lernst du
* wie man die Funktion `pipeline()` benutzt, um NLP-Aufgabenstellungen wie Textgenerierung und Klassifikation zu l√∂sen,
* mehr √ºber die Transformer-Architektur und
* wie zwischen Encoder-, Decoder- und Encoder-Decoder-Architekturen und -Anwendungsf√§llen unterschieden werden kann.
