# Zusammenfassung

In diesem Kapitel hast du gelernt, wie du verschiedene NLP-Aufgaben mit der High-Level-Funktion `pipeline()` aus der ü§ó Transformers-Bibliothek angehen kannst. Du hast auch erfahren, wie du im Hub nach Modellen suchen und sie nutzen kannst, und wie du die Inference API verwenden kannst, um die Modelle direkt in deinem Browser zu testen.

Wir haben besprochen, wie Transformer-Modelle im Gro√üen und Ganzen funktionieren, und haben die Bedeutung von Tranfer Learning und Feintuning erl√§utert. Ein wichtiger Aspekt ist, dass du entweder die gesamte Architektur, nur den Encoder oder auch nur den Decoder verwenden kannst - je nachdem, welche Art von Aufgabe du l√∂sen willst. Die nachfolgende Tabelle gibt noch einmal einen guten √úberblick:

| Modell          | Beispiele                                  | Aufgaben (Tasks)                                                                            |
|-----------------|--------------------------------------------|----------------------------------------------------------------------------------|
| Encoder         | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Klassifizierung von S√§tzen, Eigennamenerkennung/NER, Extraktives Question Answering |
| Decoder         | CTRL, GPT, GPT-2, Transformer XL           | Textgenerierung                                                                  |
| Encoder-Decoder | BART, T5, Marian, mBART                    | Automatische Textzusammenfassung, Maschinelle √úbersetzung, Generatives Question Answering                        |
