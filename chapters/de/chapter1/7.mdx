# Sequence-to-Sequence-Modelle

<Youtube id="0_4KEb08xrE" />

Encoder-Decoder-Modelle (auch *Sequence-to-Sequence-Modelle* genannt) verwenden beide Teile der Transformer-Architektur. Die Attention Layer des Encoders können in jeder Phase(#M#) auf alle Wörter des Ausgangssatzes zugreifen, während die Attention Layer des Decoders nur auf die Wörter zugreifen können, die vor einem bestimmten Wort des Inputs stehen.

Das Pretraining dieser Modelle kann anhand der Zielfunktionen(#M Optimierungs.., using objectives of #) von Encoder- oder Decoder-Modellen erfolgen, ist aber in der Regel etwas komplexer. Beim Pretraining von [T5](https://huggingface.co/t5-base) werden zum Beispiel zufällige Textabschnitte (die mehrere Wörter enthalten können) durch ein einzelnes spezielles Maskierungswort(#M Maskenwort#) ersetzt, und das Ziel besteht dann darin, den Text vorherzusagen, der durch dieses Maskierungswort(#M Maskenwort#) ersetzt wurde.

Sequence-to-Sequence-Modelle eignen sich am besten für Aufgaben, bei denen es darum geht, neue Sätze in Abhängigkeit von einem bestimmten Input zu generieren, z. B. bei der Zusammenfassung, Übersetzung oder generativen Beantwortung von Fragen (Generative Question Answering).

Vertreter dieser Modellfamilie sind u. a.:

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)
