# Decoder-Modelle

<Youtube id="d_ixlCubqQw" />

Decoder-Modelle verwenden nur den Decoder eines Transformer-Modells. Die Attention Layer können in jeder Phase(#M zu jeder Zeit#) für ein bestimmtes Wort(#M# hinsichtlich eines bestimmten Wortes #M#) nur auf die Wörter zugreifen, die vor diesem Wort im Satz stehen. Diese Modelle werden oft als *autoregressive Modelle* bezeichnet.

Beim Pretraining von Decoder-Modellen geht es in der Regel um die Vorhersage des nächsten Wortes im Satz.

Diese Modelle sind am besten für Aufgaben geeignet, bei denen es um die Generierung von Texten geht.

Zu dieser Modellfamilie gehören unter anderem:

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transformerxl.html)
