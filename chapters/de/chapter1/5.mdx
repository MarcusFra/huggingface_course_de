# Encoder-Modelle

<Youtube id="MUqNwgPjJvQ" />

Encoder-Modelle verwenden nur den Encoder eines Transformer-Modells. Die Attention Layer können in jeder Phase(#M zu jeder Zeit#) auf alle Wörter des Ausgangssatzes zugreifen. Diese Modelle werden häufig als Modelle mit "bidirektionaler" (engl. bi-directional) Attention bezeichnet und oft *Auto-Encoding-Modelle* genannt.

Beim Pretraining dieser Modelle geht es in der Regel darum, einen bestimmten Satz auf irgendeine Weise zu verfälschen (z. B. indem zufällig Wörter darin maskiert werden) und das Modell dann damit zu betrauen, den ursprünglichen Satz zu finden bzw. zu rekonstruieren.

Encoder-Modelle eignen sich am besten für Aufgaben, die ein Verständnis des gesamten Satzes erfordern, wie z. B. die Klassifizierung von Sätzen, die Eigennamenerkennung (bzw. allgemeiner die Klassifikation von Wörtern) und die extraktive(#M#) Beantwortung von Fragen (Extractive Question Answering).

Zu dieser Modellfamilie gehören unter anderem:

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
